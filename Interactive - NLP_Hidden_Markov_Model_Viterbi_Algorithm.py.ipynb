{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "isInteractiveWindowMessageCell": true
   },
   "source": [
    "Connected to miniconda3 (Python 3.11.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **POS Tagger Prediction Explaination**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Luopeiwen (Tina) Yi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Table of Content**\n",
    "+ ####  Main Code and Output\n",
    "+ ####  General Observation \n",
    "+ ####  Investigation of Wrong Prediction\n",
    "+ ####  Summary Report\n",
    "  + General Observation \n",
    "  + Code Overview \n",
    "  + The Correct POS Prediction Explaination\n",
    "  + The Incorrect POS Prediction Explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Main Code and Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test sentence: ['Those', 'coming', 'from', 'other', 'denominations', 'will', 'welcome', 'the', 'opportunity', 'to', 'become', 'informed', '.']\n",
      "Predicted tags: ['DET', 'NOUN', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'VERB', '.']\n",
      "Actual tags: ['DET', 'VERB', 'ADP', 'ADJ', 'NOUN', 'VERB', 'VERB', 'DET', 'NOUN', 'PRT', 'VERB', 'VERB', '.']\n",
      "Accuracy: 0.9230769230769231\n",
      "Accuracy in Percentage: 92.31 %\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test sentence: ['The', 'preparatory', 'class', 'is', 'an', 'introductory', 'face-to-face', 'group', 'in', 'which', 'new', 'members', 'become', 'acquainted', 'with', 'one', 'another', '.']\n",
      "Predicted tags: ['DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'NOUN', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'NUM', 'NOUN', '.']\n",
      "Actual tags: ['DET', 'ADJ', 'NOUN', 'VERB', 'DET', 'ADJ', 'ADJ', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'VERB', 'VERB', 'ADP', 'NUM', 'DET', '.']\n",
      "Accuracy: 0.8888888888888888\n",
      "Accuracy in Percentage: 88.89 %\n",
      "\n",
      "==================================================\n",
      "\n",
      "Test sentence: ['It', 'provides', 'a', 'natural', 'transition', 'into', 'the', 'life', 'of', 'the', 'local', 'church', 'and', 'its', 'organizations', '.']\n",
      "Predicted tags: ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'CONJ', 'DET', 'NOUN', '.']\n",
      "Actual tags: ['PRON', 'VERB', 'DET', 'ADJ', 'NOUN', 'ADP', 'DET', 'NOUN', 'ADP', 'DET', 'ADJ', 'NOUN', 'CONJ', 'DET', 'NOUN', '.']\n",
      "Accuracy: 1.0\n",
      "Accuracy in Percentage: 100.0 %\n",
      "\n",
      "==================================================\n",
      "\n",
      "Overall Accuracy for the three sentences: 0.9361702127659575\n",
      "Overall Accuracy for the three sentences in percentage: 93.62%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Viterbi Algorithm for inferring the most likely sequence of states from an HMM.\n",
    "Patrick Wang, 2021\n",
    "\"\"\"\n",
    "\n",
    "from typing import Sequence, Tuple, TypeVar\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# nltk.download(\"brown\")\n",
    "# nltk.download(\"universal_tagset\")\n",
    "\n",
    "\n",
    "def getWordTag(corpus):\n",
    "    words = set()\n",
    "    tags = set()\n",
    "    for sentence in corpus:\n",
    "        for word, tag in sentence:\n",
    "            words.add(word)\n",
    "            tags.add(tag)\n",
    "    # Add OOV token to word list\n",
    "    words.add(\"OOV\")\n",
    "    # Convert sets to lists for index-based operations\n",
    "    word_map = list(words)\n",
    "    tag_map = list(tags)\n",
    "    return word_map, tag_map\n",
    "\n",
    "\n",
    "def genMatrix(corpus, word_map, tag_map):\n",
    "    num_tags = len(tag_map)\n",
    "    num_words = len(word_map)\n",
    "    # Create dictionaries for efficient index lookup\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(word_map)}\n",
    "    tag_to_idx = {tag: idx for idx, tag in enumerate(tag_map)}\n",
    "    # 1. Initial State Distribution Matrix\n",
    "    initState = np.ones(num_tags)  # Laplace Smoothing\n",
    "    for sentence in corpus:\n",
    "        _, tag = sentence[0]  # consider only the first word of each sentence\n",
    "        initState[tag_to_idx[tag]] += 1\n",
    "    initState /= initState.sum()  # Normalize\n",
    "    # 2. Transition Matrix\n",
    "    matState = np.ones((num_tags, num_tags))  # Laplace Smoothing\n",
    "    for sentence in corpus:\n",
    "        for i in range(len(sentence) - 1):\n",
    "            curr_tag_idx = tag_to_idx[sentence[i][1]]\n",
    "            next_tag_idx = tag_to_idx[sentence[i + 1][1]]\n",
    "            matState[curr_tag_idx][next_tag_idx] += 1\n",
    "    # Normalize each row\n",
    "    matState /= matState.sum(axis=1, keepdims=True)\n",
    "    # 3. Observation Matrix\n",
    "    matObs = np.ones((num_tags, num_words))  # Laplace Smoothing\n",
    "    for sentence in corpus:\n",
    "        for word, tag in sentence:\n",
    "            matObs[tag_to_idx[tag]][word_to_idx[word]] += 1\n",
    "    # Normalize each row\n",
    "    matObs /= matObs.sum(axis=1, keepdims=True)\n",
    "    return matState, matObs, initState\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "\n",
    "training = nltk.corpus.brown.tagged_sents(tagset=\"universal\")[:10000]\n",
    "word_map, tag_map = getWordTag(training)\n",
    "matState, matObs, initState = genMatrix(training, word_map, tag_map)\n",
    "# print(\"Initial State (initState):\", initState)\n",
    "# print(\"Transition Matrix (matState):\", matState)\n",
    "# print(\"Observation Matrix (matObs):\", matObs)\n",
    "# Q represents the states\n",
    "# V represents the observations\n",
    "\n",
    "Q = TypeVar(\"Q\")\n",
    "V = TypeVar(\"V\")\n",
    "# obs: A sequence of observed values (usually word indices).\n",
    "# pi: Initial state probabilities.\n",
    "# A: State transition probabilities.\n",
    "# B: Emission probabilities.\n",
    "# It returns a tuple containing the most probable sequence of states (qs) and its probability.\n",
    "\n",
    "\n",
    "def viterbi(\n",
    "    obs: Sequence[int],\n",
    "    pi: np.ndarray[Tuple[V], np.dtype[np.float_]],\n",
    "    A: np.ndarray[Tuple[Q, Q], np.dtype[np.float_]],\n",
    "    B: np.ndarray[Tuple[Q, V], np.dtype[np.float_]],\n",
    ") -> tuple[list[int], float]:\n",
    "    \"\"\"Infer most likely state sequence using the Viterbi algorithm.\n",
    "    Args:\n",
    "        obs: An iterable of ints representing observations.\n",
    "        pi: A 1D numpy array of floats representing initial state probabilities.\n",
    "        A: A 2D numpy array of floats representing state transition probabilities.\n",
    "        B: A 2D numpy array of floats representing emission probabilities.\n",
    "    Returns:\n",
    "        A tuple of:\n",
    "        * A 1D numpy array of ints representing the most likely state sequence.\n",
    "        * A float representing the probability of the most likely state sequence.\n",
    "    \"\"\"\n",
    "    # N: The number of observations.\n",
    "    # Q: The number of states.\n",
    "    # V: The number of unique observations.\n",
    "    N = len(obs)\n",
    "    Q, V = B.shape  # num_states, num_observations\n",
    "    # d_{ti} = max prob of being in state i at step t\n",
    "    #   AKA viterbi\n",
    "    # \\psi_{ti} = most likely state preceeding state i at step t\n",
    "    #   AKA backpointer\n",
    "    # initialization\n",
    "    log_d = [np.log(pi) + np.log(B[:, obs[0]])]\n",
    "    log_psi = [np.zeros((Q,))]\n",
    "    # recursion\n",
    "    for z in obs[1:]:\n",
    "        log_da = np.expand_dims(log_d[-1], axis=1) + np.log(A)\n",
    "        log_d.append(np.max(log_da, axis=0) + np.log(B[:, z]))\n",
    "        log_psi.append(np.argmax(log_da, axis=0))\n",
    "    # termination\n",
    "    log_ps = np.max(log_d[-1])\n",
    "    qs = [-1] * N\n",
    "    qs[-1] = int(np.argmax(log_d[-1]))\n",
    "    for i in range(N - 2, -1, -1):\n",
    "        qs[i] = log_psi[i + 1][qs[i + 1]]\n",
    "    return qs, np.exp(log_ps)\n",
    "\n",
    "\n",
    "def words_to_indices(words, word_map):\n",
    "    \"\"\"Convert a list of words to their respective indices.\"\"\"\n",
    "    return [\n",
    "        word_map.index(word) if word in word_map else word_map.index(\"OOV\")\n",
    "        for word in words\n",
    "    ]\n",
    "\n",
    "\n",
    "# Extracting test data\n",
    "\n",
    "test_data = nltk.corpus.brown.tagged_sents(tagset=\"universal\")[10150:10153]\n",
    "# Function to compute accuracy\n",
    "\n",
    "\n",
    "def accuracy(predicted, actual):\n",
    "    return sum(p == a for p, a in zip(predicted, actual)) / len(predicted)\n",
    "\n",
    "\n",
    "total_correct_tags = 0\n",
    "total_tags = 0\n",
    "# Looping over each sentence in the test data\n",
    "\n",
    "for test_sentence in test_data:\n",
    "    words = [\n",
    "        word for word, _ in test_sentence\n",
    "    ]  # Extracting words from the (word, tag) tuples\n",
    "    obs_indices = words_to_indices(words, word_map)  # Convert words to indices\n",
    "    state_sequence, _ = viterbi(obs_indices, initState, matState, matObs)\n",
    "    predicted_tags = [tag_map[idx] for idx in state_sequence]\n",
    "    actual_tags = [tag for _, tag in test_sentence]\n",
    "    total_correct_tags += sum(p == a for p, a in zip(predicted_tags, actual_tags))\n",
    "    total_tags += len(actual_tags)\n",
    "    acc = accuracy(predicted_tags, actual_tags)\n",
    "    print(\"Test sentence:\", words)\n",
    "    print(\"Predicted tags:\", predicted_tags)\n",
    "    print(\"Actual tags:\", actual_tags)\n",
    "    print(\"Accuracy:\", acc)\n",
    "    print(\"Accuracy in Percentage:\", round(acc * 100, 2), \"%\")\n",
    "    print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "overall_accuracy = total_correct_tags / total_tags\n",
    "print(f\"Overall Accuracy for the three sentences: {overall_accuracy}\")\n",
    "print(\n",
    "    f\"Overall Accuracy for the three sentences in percentage: {overall_accuracy * 100:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **General Observation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Overall, my model's POS tagger prediction achieve a 93.62% accuracy. \n",
    "\n",
    "+ The accuracy of the first sentence POS tagger prediction is around 92.31 %. In the first sentence, the word \"coming\" was predicted as a 'NOUN' but the actual tag is 'VERB'.\n",
    "\n",
    "+ The accuracy of the second sentence POS tagger prediction is around 88.89 %. In the second sentence, the word \"face-to-face\" was predicted as a 'NOUN', but the actual tag is 'ADJ'. Additionally, \"another\" was predicted as 'NOUN', but the actual tag is 'DET'.\n",
    "\n",
    "+ The accuracy of the first sentence POS tagger prediction is 100%. In the third sentence, the tags predicted by the model match perfectly with the actual tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Investigation of Wrong Prediction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 'coming' appears in the training data with the following tags: VERB, NOUN.\n",
      "The word 'face-to-face' does not appear in the training data. It is an OOV token.\n",
      "The word 'another' appears in the training data with the following tags: DET.\n"
     ]
    }
   ],
   "source": [
    "# Check whether the words \"coming\", \"face-to-face\", and \"another\" are present in the training data and check their word tags\n",
    "# Words to check\n",
    "words_to_check = [\"coming\", \"face-to-face\", \"another\"]\n",
    "\n",
    "# Create a dictionary to store tags for each word\n",
    "word_tags = {word: set() for word in words_to_check}\n",
    "\n",
    "# Loop through the training data to gather tags\n",
    "for sentence in training:\n",
    "    for word, tag in sentence:\n",
    "        if word in words_to_check:\n",
    "            word_tags[word].add(tag)\n",
    "\n",
    "# Display results\n",
    "for word, tags in word_tags.items():\n",
    "    if tags:\n",
    "        print(\n",
    "            f\"The word '{word}' appears in the training data with the following tags: {', '.join(tags)}.\"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"The word '{word}' does not appear in the training data. It is an OOV token.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General Observation:\n",
    "\n",
    "The overall accuracy of the POS tagger is 93.62%. This indicates that in the majority of cases, the model correctly predicted the POS tags. This is a very good result considering perfect accuracy in POS tagging is challenging due to the intricacies of human language and the limitation of training set. Here is a more detailed explaination of why my POS tagger does or does not produce the correct tags. \n",
    "\n",
    "---\n",
    "\n",
    "#### Code Overview:\n",
    "\n",
    "**1. Word and Tag Mapping:**\n",
    "\n",
    "**Function**: `getWordTag`\n",
    "\n",
    "This function extracts all distinct words and tags from the corpus. To accommodate words that didn't appear during training, I included an \"OOV\" (Out-Of-Vocabulary) token.\n",
    "\n",
    "**2. Generating Matrices for HMM:**\n",
    "\n",
    "**Function**: `genMatrix`\n",
    "\n",
    "Three matrices play pivotal roles in the Hidden Markov Model:\n",
    "\n",
    "- `initState`: Represents the initial probabilities for each tag at the beginning of a sentence.\n",
    "  \n",
    "- `matState`: Acts as the transition matrix, indicating the probabilities of transitioning from one tag to the next.\n",
    "\n",
    "- `matObs`: As the observation matrix, it signifies the likelihood of observing a specific word given a particular tag.\n",
    "\n",
    "**(1). Initial State Distribution Matrix:**\n",
    "\n",
    "- **Objective**: Denote the probabilities for each tag's appearance at a sentence's start.\n",
    "\n",
    "- **Procedure**:\n",
    "  1. Generate a 1xN matrix (N being the total unique tags).\n",
    "  2. Apply Laplace Smoothing by setting all matrix values to 1.\n",
    "  3. Loop through the corpus. For each first word in a sentence, increment the count of its associated tag.\n",
    "  4. Normalize the counts by dividing them by the total sum, ensuring the sum of probabilities equals 1.\n",
    "\n",
    "**(2). Transition Matrix:**\n",
    "\n",
    "- **Objective**: Denote the probabilities of transitioning from one tag to another.\n",
    "\n",
    "- **Procedure**:\n",
    "  1. Construct an NxN matrix based on the unique tag count (N being the total unique tags).\n",
    "  2. Apply Laplace Smoothing by setting all matrix values to 1.\n",
    "  3. Loop through the corpus. For every pair of consecutive tags, increment the count in the corresponding cell in the matrix.\n",
    "  4. Normalize rows by dividing values by their row's sum, ensuring the sum of probabilities at 1 for each row.\n",
    "\n",
    "**(3). Observation Matrix:**\n",
    "\n",
    "- **Objective**: Denote the probabilities of a word being emitted under a given tag.\n",
    "\n",
    "- **Procedure**:\n",
    "  1. Frame an NxM matrix (N being the total unique tag count, M being the total unique word count).\n",
    "  2. Apply Laplace Smoothing by setting all matrix values to 1.\n",
    "  3. Loop through the corpus. For every word-tag pair, increment the corresponding cell in the matrix.\n",
    "  4. Normalize rows by dividing values by their row's sum, ensuring the sum of probabilities at 1 for each row.\n",
    "\n",
    "**3. Viterbi Algorithm:**\n",
    "\n",
    "**Function**: `viterbi`\n",
    "\n",
    "For a provided word sequence (observations), this algorithm determines the most probable sequence of POS tags (states) leveraging the aforementioned matrices. It employs dynamic programming to ascertain the likelihood of each word being in a specific state, factoring in the previous state. To deduce the optimal state sequence (tags) for a word sequence, backtracking is utilized.\n",
    "\n",
    "**4. Testing and Evaluating Accuracy:**\n",
    "\n",
    "The code subjects each test sentence to the following procedure:\n",
    "\n",
    "1. Words undergo a conversion to their respective indices as per `word_map`. Unfamiliar words get substituted with the \"OOV\" index.\n",
    "2. Utilizing the Viterbi algorithm on the indexed sentence yields the most probable tag sequence.\n",
    "3. For accuracy computation, the system contrasts the predicted tags against the actual ones.\n",
    "\n",
    "---\n",
    "#### The Correct POS Prediction Explaination\n",
    "\n",
    "My code used a Hidden Markov Model (HMM) for POS tagging. Utilizing probability matrices derived from training data and the Viterbi algorithm, it determines the likeliest tags for word sequences. The model is considered to be successful because of the diversity of the training data, and the correct implementation of the matrix-generation methods and Viterbi algorithm.\n",
    "\n",
    "---\n",
    "#### The Incorrect POS Prediction Explaination\n",
    "\n",
    "1. **First Sentence:** \n",
    "   - Accuracy is approximately 92.31%.\n",
    "   - The word \"coming\" was predicted as a 'NOUN' but was actually a 'VERB'. \n",
    "   - **Analysis:** Upon checking the training data, it was found that the word \"coming\" has been tagged both as 'VERB' and 'NOUN'. In some contexts, \"coming\" might be used as a noun (e.g., \"The coming of winter\"), while in others, it's a verb (e.g., \"She is coming to the party\"). The model seems to have been influenced by the dual nature of this word in the training data, leading to a potential misclassification in this specific context. \n",
    "\n",
    "2. **Second Sentence:**\n",
    "   - Accuracy is around 88.89%.\n",
    "   - The word \"face-to-face\" was predicted as 'NOUN', but the actual tag is 'ADJ'.\n",
    "   - The word \"another\" was predicted as 'NOUN', but the actual tag is 'DET'.\n",
    "   - **Analysis:** The word \"face-to-face\" was not found in the training data. This implies it's an OOV (Out-Of-Vocabulary) word for the model. Hence, any prediction for this word would be based on the model's generalizations and might not be accurate. For \"another,\" the training data showed it has been tagged as 'DET', but the model incorrectly predicted it as a 'NOUN'. This could be attributed to the model's bias towards more frequent patterns or contexts in the training data where similar words appear as nouns.\n",
    "\n",
    "3. **Third Sentence:**\n",
    "   - Accuracy is 100%.\n",
    "   - **Analysis:** This suggests that the words in this sentence and their respective contexts were well-represented or resembled patterns seen in the training data. The model made accurate predictions in this instance.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The model's errors can be attributed to three main factors:\n",
    "1. Ambiguity of certain words in the training data (words can serve different roles based on context), as seen with the word \"coming\".\n",
    "2. Model's bias towards more frequent patterns or contexts in the training data where similar words appear as a different tag, as seen with the word \"another\".\n",
    "3. Absence of certain words from the training data, leading to possibly inaccurate generalizations, as observed with \"face-to-face\".\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
